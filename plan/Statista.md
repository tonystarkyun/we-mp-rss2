# Statista å®šåˆ¶åŒ–çˆ¬å–å®ç°è¯¦è§£

## å®ç°æ¦‚è¿°

### èƒŒæ™¯é—®é¢˜
Statistaï¼ˆå…¨çƒé¢†å…ˆçš„å¸‚åœºå’Œæ¶ˆè´¹æ•°æ®å¹³å°ï¼‰çš„æœç´¢é¡µé¢å­˜åœ¨ä»¥ä¸‹æŠ€æœ¯æŒ‘æˆ˜ï¼š

1. **å‰ç«¯æ¸²æŸ“ä¾èµ–**: æœç´¢ç»“æœé€šè¿‡JavaScriptåŠ¨æ€ç”Ÿæˆï¼ŒHTMLæºç ä¸­ä¸åŒ…å«å®é™…ç»Ÿè®¡æ•°æ®
2. **å¯¼èˆªé“¾æ¥å¹²æ‰°**: é€šç”¨é€‰æ‹©å™¨ä¼šè¯¯æŠ“å–å¤§é‡å¯¼èˆªæ ã€åˆ†ç±»é¡µé¢ç­‰éç»Ÿè®¡å†…å®¹é“¾æ¥
3. **å¤æ‚URLç»“æ„**: ä¸åŒç±»å‹çš„ç»Ÿè®¡æ•°æ®ï¼ˆå›¾è¡¨ã€ç ”ç©¶ã€è¯é¢˜ç­‰ï¼‰ä½¿ç”¨ä¸åŒçš„URLè·¯å¾„ç»“æ„
4. **å®ä½“ç±»å‹è¯†åˆ«**: éœ€è¦æ ¹æ®å†…å®¹ç±»å‹æ„å»ºæ­£ç¡®çš„è¯¦æƒ…é¡µé“¾æ¥

### è§£å†³æ–¹æ¡ˆ
é‡‡ç”¨"å†…éƒ¨API + æ™ºèƒ½URLæ„å»º"ç­–ç•¥ï¼š
1. **APIå‘ç°**: è¯†åˆ«Statistaå†…éƒ¨ä½¿ç”¨çš„æœç´¢JSONæ¥å£
2. **æµè§ˆå™¨å†…è°ƒç”¨**: é€šè¿‡`fetch`åœ¨é¡µé¢ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨æ¥å£ï¼Œç»•è¿‡CORSé™åˆ¶
3. **å®ä½“ç±»å‹æ˜ å°„**: æ ¹æ®APIè¿”å›çš„å®ä½“ä¿¡æ¯æ™ºèƒ½æ„å»ºURL
4. **å¤šå­—æ®µæ ‡é¢˜æå–**: ä¼˜å…ˆçº§çº§è”æå–æœ€ä½³æ ‡é¢˜å†…å®¹

## æŠ€æœ¯æ¶æ„

### å‡½æ•°è°ƒç”¨é“¾

```
crawl_website(url, max_articles)
â”œâ”€â”€ crawler_instance.crawl_website_articles()
â”‚   â”œâ”€â”€ async_playwright() å¯åŠ¨æµè§ˆå™¨
â”‚   â”œâ”€â”€ page.goto(url) è®¿é—®æœç´¢é¡µé¢
â”‚   â””â”€â”€ _extract_articles(page, base_url, max_articles)
â”‚       â””â”€â”€ urlparse(base_url) è§£æURL
â”‚           â””â”€â”€ æ£€æµ‹statista.com + /searchè·¯å¾„
â”‚               â””â”€â”€ _extract_statista_search(page, base_url, max_articles)
â”‚                   â”œâ”€â”€ parse_qs() è§£ææŸ¥è¯¢å‚æ•°
â”‚                   â”œâ”€â”€ æ„å»ºAPIå‚æ•°ï¼ˆä¿ç•™åŸå‚æ•° + asJsonResponse=1ï¼‰
â”‚                   â”œâ”€â”€ è¡¥å……é»˜è®¤å‚æ•°ï¼ˆq, p, sortMethodç­‰ï¼‰
â”‚                   â”œâ”€â”€ page.evaluate() æµè§ˆå™¨å†…fetchè°ƒç”¨
â”‚                   â”œâ”€â”€ json.loads() è§£æAPIå“åº”
â”‚                   â”œâ”€â”€ å®ä½“IDæ˜ å°„å’Œç±»å‹è¯†åˆ«
â”‚                   â”œâ”€â”€ _build_statista_url() æ™ºèƒ½URLæ„å»º
â”‚                   â””â”€â”€ å¤šå­—æ®µæ ‡é¢˜æå–å’Œè®°å½•æ ‡å‡†åŒ–
```

### æ ¸å¿ƒç­–ç•¥

1. **å‚æ•°é€ä¼ **: ä¿ç•™ç”¨æˆ·åŸå§‹æœç´¢å‚æ•°ï¼Œç¡®ä¿æœç´¢æ„å›¾ä¸å˜
2. **é»˜è®¤è¡¥é½**: ä¸ºç¼ºå¤±çš„å…³é”®å‚æ•°æä¾›åˆç†é»˜è®¤å€¼
3. **å®ä½“æ˜ å°„**: é€šè¿‡`justSmart.actionParameters.entityIds`å»ºç«‹IDåˆ°ç±»å‹çš„æ˜ å°„
4. **URLæ™ºèƒ½æ„å»º**: æ ¹æ®å®ä½“ç±»å‹é€‰æ‹©æ­£ç¡®çš„è·¯å¾„å‰ç¼€

## æ ¸å¿ƒå®ç°

### 1. åŸŸåæ£€æµ‹ä¸è·¯ç”± (`core/crawler.py:789-792`)

```python
if parsed_url.netloc.endswith('statista.com') and parsed_url.path.startswith('/search'):
    statista_articles = await self._extract_statista_search(page, base_url, max_articles)
    if statista_articles:
        return statista_articles
```

**è§¦å‘æ¡ä»¶**:
- **åŸŸååŒ¹é…**: `*.statista.com`
- **è·¯å¾„åŒ¹é…**: `/search*` (æœç´¢é¡µé¢ä¸“ç”¨)
- **ä¼˜å…ˆçº§**: è¾ƒä½ä¼˜å…ˆçº§ (åœ¨ä¸‡æ–¹ã€FoodNavigatorã€Reutersã€Panda985ä¹‹å)

### 2. æŸ¥è¯¢å‚æ•°å¤„ç† (`core/crawler.py:519-527`)

```python
parsed = urlparse(base_url)
query_params = parse_qs(parsed.query)

# æ„å»ºè¯·æ±‚å‚æ•°ï¼Œä¿ç•™åŸæœ‰æŸ¥è¯¢æ¡ä»¶å¹¶å¼ºåˆ¶è¿”å› JSON
api_params = [('asJsonResponse', '1')]
for key, values in query_params.items():
    for value in values:
        if value is not None:
            api_params.append((key, value))
```

**å‚æ•°å¤„ç†ç­–ç•¥**:
- **é€ä¼ åŸå‚æ•°**: ä¿ç•™ç”¨æˆ·æ‰€æœ‰æœç´¢æ¡ä»¶
- **å¼ºåˆ¶JSON**: æ·»åŠ `asJsonResponse=1`æ¿€æ´»APIæ¨¡å¼
- **å¤šå€¼æ”¯æŒ**: æ­£ç¡®å¤„ç†å…·æœ‰å¤šä¸ªå€¼çš„å‚æ•°

**æ”¯æŒçš„æœç´¢å‚æ•°**:
- `q`: æœç´¢å…³é”®è¯
- `p`: é¡µç 
- `sortMethod`: æ’åºæ–¹å¼ (`relevance`, `publicationDate`)
- `accuracy`: æœç´¢ç²¾åº¦ (`and`, `or`)
- `interval`: æ—¶é—´åŒºé—´
- `isoregion`: åœ°ç†åŒºåŸŸ
- `language`: è¯­è¨€è®¾ç½®

### 3. é»˜è®¤å‚æ•°è¡¥é½ (`core/crawler.py:529-544`)

```python
defaults = {
    'q': '',                    # æœç´¢å…³é”®è¯
    'p': '1',                   # é¡µç 
    'sortMethod': 'relevance',  # æ’åºæ–¹å¼
    'accuracy': 'and',          # æœç´¢ç²¾åº¦
    'interval': '0',            # æ—¶é—´åŒºé—´
    'idRelevance': '0',         # IDç›¸å…³æ€§
    'isRegionPref': '-1',       # åœ°åŒºåå¥½
    'isoregion': '0',           # ISOåœ°åŒºä»£ç 
    'language': '0',            # è¯­è¨€è®¾ç½®
}
existing_keys = {key for key, _ in api_params}
for key, value in defaults.items():
    if key not in existing_keys:
        api_params.append((key, value))
```

**é»˜è®¤å€¼è®¾è®¡åŸç†**:
- **APIå…¼å®¹æ€§**: Statistaæ¥å£åœ¨ç¼ºå°‘å‚æ•°æ—¶ä¼šä½¿ç”¨å†…éƒ¨é»˜è®¤å€¼ï¼Œå¯èƒ½ä¸é¢„æœŸä¸ç¬¦
- **æœç´¢è´¨é‡**: æ˜ç¡®æŒ‡å®šå‚æ•°ç¡®ä¿æœç´¢ç»“æœçš„ä¸€è‡´æ€§
- **å‘åå…¼å®¹**: å³ä½¿APIæ›´æ–°ï¼Œé»˜è®¤å‚æ•°ä¹Ÿèƒ½æä¾›åŸºç¡€åŠŸèƒ½

### 4. å†…éƒ¨APIè°ƒç”¨ (`core/crawler.py:546-562`)

```python
query_string = urlencode(api_params, doseq=True)
target_url = f"https://www.statista.com/search/?{query_string}"
fetch_result = await page.evaluate(
    "async (targetUrl) => {\n                    const response = await fetch(targetUrl, {\n                        headers: {\n                            'Accept': 'application/json, text/plain, */*',\n                            'X-Requested-With': 'XMLHttpRequest'\n                        },\n                        credentials: 'include'\n                    });\n                    const text = await response.text();\n                    return { status: response.status, text };\n                }",
    target_url
)
```

**æŠ€æœ¯è¦ç‚¹**:

**APIç«¯ç‚¹**: ä¸ç”¨æˆ·è®¿é—®çš„URLç›¸åŒï¼Œä½†é€šè¿‡`asJsonResponse=1`åˆ‡æ¢åˆ°JSONæ¨¡å¼

**å…³é”®è¯·æ±‚å¤´**:
- `Accept`: æ˜ç¡®æœŸæœ›JSONå“åº”
- `X-Requested-With`: æ ‡è¯†AJAXè¯·æ±‚ï¼Œè§¦å‘APIå“åº”æ¨¡å¼
- `credentials: 'include'`: åŒ…å«ç”¨æˆ·ä¼šè¯ä¿¡æ¯

**æµè§ˆå™¨å†…æ‰§è¡Œä¼˜åŠ¿**:
- **ä¼šè¯ç»§æ‰¿**: è‡ªåŠ¨ä½¿ç”¨é¡µé¢çš„ç™»å½•çŠ¶æ€å’ŒCookie
- **CORSç»•è¿‡**: é¿å…è·¨åŸŸè¯·æ±‚é™åˆ¶
- **ç¯å¢ƒä¸€è‡´**: ä¸æ­£å¸¸ç”¨æˆ·è®¿é—®ç¯å¢ƒå®Œå…¨ç›¸åŒ

### 5. æ•°æ®ç»“æ„è§£æ (`core/crawler.py:563-566`)

```python
data = json.loads(raw_text)
results = data.get('results', {})
main_results = results.get('mainselect') or []
entity_ids = data.get('justSmart', {}).get('actionParameters', {}).get('entityIds', {})
id_to_name = {str(v): k for k, v in entity_ids.items()}
```

**APIå“åº”ç»“æ„**:
```json
{
    "results": {
        "mainselect": [
            {
                "identity": "123456",
                "graphheader": "ä¸»æ ‡é¢˜",
                "pagetitle": "é¡µé¢æ ‡é¢˜", 
                "catchline": "å‰¯æ ‡é¢˜",
                "title": "æ ‡é¢˜",
                "uri": "url-slug",
                "idcontent": "content-id"
            }
        ]
    },
    "justSmart": {
        "actionParameters": {
            "entityIds": {
                "statistic": 123456,
                "topic": 789012
            }
        }
    }
}
```

**æ•°æ®æ˜ å°„é€»è¾‘**:
- **ä¸»è¦ç»“æœ**: `results.mainselect`åŒ…å«æœç´¢åˆ°çš„ç»Ÿè®¡é¡¹ç›®
- **å®ä½“ç±»å‹**: `justSmart.actionParameters.entityIds`æä¾›IDåˆ°ç±»å‹çš„æ˜ å°„
- **åå‘æ˜ å°„**: æ„å»º`{id: type}`å­—å…¸ç”¨äºURLæ„å»º

### 6. å®ä½“ç±»å‹æ˜ å°„ç³»ç»Ÿ (`core/crawler.py:736-751`)

```python
entity_to_path = {
    'statistic': 'statistics',      # ç»Ÿè®¡æ•°æ®
    'forecast': 'statistics',       # é¢„æµ‹æ•°æ®  
    'infographic': 'infographic',   # ä¿¡æ¯å›¾è¡¨
    'topic': 'topics',              # ä¸»é¢˜é¡µé¢
    'study': 'study',               # ç ”ç©¶æŠ¥å‘Š
    'dossier': 'study',             # æ¡£æ¡ˆæŠ¥å‘Š
    'dossierplus': 'study',         # é«˜çº§æ¡£æ¡ˆ
    'toplist': 'study',             # æ’è¡Œæ¦œ
    'survey': 'study',              # è°ƒæŸ¥æŠ¥å‘Š
    'marketstudy': 'study',         # å¸‚åœºç ”ç©¶
    'branchreport': 'study',        # è¡Œä¸šæŠ¥å‘Š
    'brandreport': 'study',         # å“ç‰ŒæŠ¥å‘Š
    'companyreport': 'study',       # å…¬å¸æŠ¥å‘Š
    'countryreport': 'study',       # å›½å®¶æŠ¥å‘Š
}
```

**æ˜ å°„åŸç†**:
- **è·¯å¾„è§„èŒƒåŒ–**: ä¸åŒå®ä½“ç±»å‹å¯¹åº”ä¸åŒçš„URLè·¯å¾„å‰ç¼€
- **ç±»å‹èšåˆ**: å¤šç§æŠ¥å‘Šç±»å‹ç»Ÿä¸€å½’ç±»ä¸º`study`è·¯å¾„
- **URLä¸€è‡´æ€§**: ç¡®ä¿ç”Ÿæˆçš„é“¾æ¥ä¸Statistaå®˜æ–¹URLæ ¼å¼ä¸€è‡´

### 7. æ™ºèƒ½URLæ„å»º (`core/crawler.py:724-761`)

```python
def _build_statista_url(self, item: Dict, entity_name: Optional[str]) -> Optional[str]:
    if not item:
        return None

    slug = (item.get('uri') or item.get('url') or '').strip('/')
    if not slug:
        return None

    idcontent = item.get('idcontent')
    base = 'https://www.statista.com'

    # æ ¹æ®å®ä½“ç±»å‹æ„å»ºURL
    path_prefix = entity_to_path.get(entity_name)
    if path_prefix and idcontent:
        return f"{base}/{path_prefix}/{idcontent}/{slug}/"

    # å›é€€åˆ°ç›´æ¥æ‹¼æ¥slug
    if slug.startswith('http'):
        return slug
    return f"{base}/{slug}/"
```

**URLæ„å»ºé€»è¾‘**:

**å®Œæ•´è·¯å¾„æ ¼å¼**: `https://www.statista.com/{path_prefix}/{idcontent}/{slug}/`

**æ„å»ºæ­¥éª¤**:
1. **æå–slug**: ä»`uri`æˆ–`url`å­—æ®µè·å–URLç‰‡æ®µ
2. **è·å–ID**: ä»`idcontent`å­—æ®µè·å–å†…å®¹ID
3. **ç¡®å®šè·¯å¾„**: æ ¹æ®å®ä½“ç±»å‹æ˜ å°„è·å–è·¯å¾„å‰ç¼€
4. **ç»„è£…URL**: æŒ‰æ ‡å‡†æ ¼å¼ç»„è£…å®Œæ•´URL
5. **å›é€€æœºåˆ¶**: æ— æ³•ç¡®å®šç±»å‹æ—¶ç›´æ¥ä½¿ç”¨slug

**URLç¤ºä¾‹**:
- ç»Ÿè®¡æ•°æ®: `https://www.statista.com/statistics/12345/flaxseed-production/`
- ä¸»é¢˜é¡µé¢: `https://www.statista.com/topics/67890/agriculture/`
- ç ”ç©¶æŠ¥å‘Š: `https://www.statista.com/study/11111/market-analysis/`

### 8. å¤šå­—æ®µæ ‡é¢˜æå– (`core/crawler.py:578-588`)

```python
title_fields = [
    item.get('graphheader'),    # å›¾è¡¨æ ‡é¢˜ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    item.get('pagetitle'),      # é¡µé¢æ ‡é¢˜
    item.get('catchline'),      # å‰¯æ ‡é¢˜
    item.get('title'),          # é€šç”¨æ ‡é¢˜
    item.get('pseudotitle'),    # ä¼ªæ ‡é¢˜
    item.get('subtitle'),       # å­æ ‡é¢˜
]
title = next((t.strip() for t in title_fields if t), None)
if not title:
    title = article_url
```

**æ ‡é¢˜æå–ç­–ç•¥**:
- **ä¼˜å…ˆçº§çº§è”**: æŒ‰ç…§å†…å®¹è´¨é‡ä¾æ¬¡å°è¯•ä¸åŒæ ‡é¢˜å­—æ®µ
- **éç©ºéªŒè¯**: è·³è¿‡ç©ºç™½æˆ–Noneå€¼
- **URLå›é€€**: æ— å¯ç”¨æ ‡é¢˜æ—¶ä½¿ç”¨URLä½œä¸ºæ ‡é¢˜
- **é•¿åº¦é™åˆ¶**: æœ€ç»ˆæ ‡é¢˜é™åˆ¶åœ¨200å­—ç¬¦ä»¥å†…

**å­—æ®µä¼˜å…ˆçº§è¯´æ˜**:
1. `graphheader`: å›¾è¡¨ä¸“ç”¨æ ‡é¢˜ï¼Œæœ€å…·æè¿°æ€§
2. `pagetitle`: é¡µé¢SEOæ ‡é¢˜ï¼Œé€šå¸¸æœ€å®Œæ•´
3. `catchline`: è¥é”€æ ‡é¢˜ï¼Œç®€æ´æ˜äº†
4. `title`: é€šç”¨æ ‡é¢˜å­—æ®µ
5. `pseudotitle`: ç”Ÿæˆçš„æ ‡é¢˜
6. `subtitle`: è¡¥å……è¯´æ˜æ ‡é¢˜

### 9. æ•°æ®æ ‡å‡†åŒ–è¾“å‡º (`core/crawler.py:590-594`)

```python
articles.append({
    'title': title[:200],
    'url': article_url,
    'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S')
})
```

**è¾“å‡ºå­—æ®µ**:
- `title`: æ ‡é¢˜ï¼ˆæˆªæ–­ä¿æŠ¤ï¼‰
- `url`: å¯ç›´æ¥è®¿é—®çš„è¯¦æƒ…é¡µé“¾æ¥
- `extracted_at`: æ•°æ®æå–æ—¶é—´æˆ³

**è®¾è®¡ç‰¹ç‚¹**:
- **å­—æ®µç²¾ç®€**: åªä¿ç•™æ ¸å¿ƒå¿…è¦ä¿¡æ¯
- **URLå¯ç”¨æ€§**: ç¡®ä¿é“¾æ¥å¯ä»¥ç›´æ¥è®¿é—®
- **æ—¶é—´æˆ³**: ä¾¿äºæ•°æ®æ–°é²œåº¦ç®¡ç†

## APIæ¥å£è¯¦è§£

### Statista Search APIè§„èŒƒ

**ç«¯ç‚¹**: `https://www.statista.com/search/`

**è¯·æ±‚æ–¹æ³•**: GET

**å¿…éœ€å‚æ•°**:
- `asJsonResponse`: å€¼ä¸º`1`ï¼Œæ¿€æ´»JSONå“åº”æ¨¡å¼

**æœç´¢å‚æ•°**:
- `q`: æœç´¢å…³é”®è¯
- `p`: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
- `sortMethod`: æ’åºæ–¹å¼
  - `relevance`: ç›¸å…³æ€§æ’åºï¼ˆé»˜è®¤ï¼‰
  - `publicationDate`: å‘å¸ƒæ—¥æœŸæ’åº

**è¿‡æ»¤å‚æ•°**:
- `accuracy`: æœç´¢ç²¾åº¦
  - `and`: ç²¾ç¡®åŒ¹é…ï¼ˆé»˜è®¤ï¼‰
  - `or`: æ¨¡ç³ŠåŒ¹é…
- `interval`: æ—¶é—´åŒºé—´
  - `0`: ä¸é™æ—¶é—´ï¼ˆé»˜è®¤ï¼‰
  - `1`: æœ€è¿‘ä¸€å¹´
  - `2`: æœ€è¿‘äº”å¹´
- `isoregion`: åœ°ç†åŒºåŸŸä»£ç 
- `language`: è¯­è¨€è®¾ç½®
- `isRegionPref`: åœ°åŒºåå¥½è®¾ç½®

**å“åº”æ ¼å¼**:
```json
{
    "results": {
        "mainselect": [
            {
                "identity": "ç»Ÿè®¡é¡¹ID",
                "graphheader": "å›¾è¡¨æ ‡é¢˜",
                "pagetitle": "é¡µé¢æ ‡é¢˜",
                "catchline": "å‰¯æ ‡é¢˜",
                "title": "é€šç”¨æ ‡é¢˜",
                "pseudotitle": "ç”Ÿæˆæ ‡é¢˜",
                "subtitle": "å­æ ‡é¢˜",
                "uri": "url-slug",
                "url": "url-path", 
                "idcontent": "å†…å®¹ID"
            }
        ]
    },
    "justSmart": {
        "actionParameters": {
            "entityIds": {
                "å®ä½“ç±»å‹å": å®ä½“ID
            }
        }
    }
}
```

## éªŒè¯æ–¹æ³•

### å¿«é€ŸéªŒè¯
è¿è¡ŒéªŒè¯è„šæœ¬ï¼š
```bash
python verify_statista.py
```

### éªŒè¯è„šæœ¬ç¤ºä¾‹
```python
import asyncio
from core.crawler import LinkCrawler

async def main():
    crawler = LinkCrawler(headless=True, timeout=120000)
    url = 'https://www.statista.com/search/?q=flaxseed&p=1&sortMethod=publicationDate'
    result = await crawler.crawl_website_articles(url, max_articles=5)
    
    print('success:', result['success'])
    print('total_found:', result['total_found'])
    print('error:', result['error'])
    
    for idx, article in enumerate(result['articles'], 1):
        print(f"{idx}. {article['title']}")
        print(f"   URL: {article['url']}")
        print()

asyncio.run(main())
```

### é¢„æœŸè¾“å‡º
```
success: True
total_found: 5
error: None
1. Seeded area of flaxseed in Canada from 1908 to 2025 (in 1,000s)
   URL: https://www.statista.com/statistics/831898/seeded-area-flaxseed-canada/

2. Harvested area of flaxseed in Canada from 1965 to 2024 (in 1,000s)
   URL: https://www.statista.com/statistics/454274/area-of-flaxseed-harvested-in-canada/

3. Farm cash receipts from flaxseed in Canada from 1971 to 2024 (in million U.S. dollars)
   URL: https://www.statista.com/statistics/449282/farm-cash-receipts-of-flaxseed-canada/
...
```

### éªŒè¯è¦ç‚¹
- [ ] **APIè°ƒç”¨æˆåŠŸ**: æ£€æŸ¥HTTP 200å“åº”
- [ ] **JSONè§£ææ­£å¸¸**: éªŒè¯å“åº”æ ¼å¼æ­£ç¡®
- [ ] **URLæ„å»ºå‡†ç¡®**: ç¡®è®¤ç”Ÿæˆçš„é“¾æ¥å¯è®¿é—®
- [ ] **æ ‡é¢˜æå–å®Œæ•´**: æ£€æŸ¥æ ‡é¢˜å†…å®¹è´¨é‡
- [ ] **å®ä½“ç±»å‹è¯†åˆ«**: éªŒè¯ä¸åŒç±»å‹çš„URLæ„å»º

## æ¶æ„å¯¹æ¯”

### ä¸å…¶ä»–å®šåˆ¶çˆ¬è™«çš„å¯¹æ¯”

| ç‰¹æ€§ | Statista | FoodNavigator | Reuters | ä¸‡æ–¹æ•°æ® | é€šç”¨çˆ¬è™« |
|------|----------|---------------|---------|----------|-----------|
| **æ•°æ®ç±»å‹** | ç»Ÿè®¡æ•°æ® | è¡Œä¸šèµ„è®¯ | æ–°é—»æŠ¥é“ | å­¦æœ¯æ–‡çŒ® | é€šç”¨ç½‘é¡µ |
| **APIé›†æˆ** | âœ… å†…éƒ¨API | âœ… Queryly | âœ… å®˜æ–¹API | âŒ DOMè§£æ | âŒ æ— API |
| **URLæ™ºèƒ½æ„å»º** | âœ… å®ä½“æ˜ å°„ | âŒ å›ºå®šæ ¼å¼ | âŒ å›ºå®šæ ¼å¼ | âœ… ç±»å‹æ˜ å°„ | âŒ ç›´æ¥æå– |
| **å¤šå­—æ®µæ ‡é¢˜** | âœ… çº§è”æå– | âœ… å¤šå­—æ®µ | âœ… å¤‡é€‰å­—æ®µ | âœ… å•å­—æ®µ | âœ… å•å­—æ®µ |
| **å‚æ•°é€ä¼ ** | âœ… å®Œæ•´ä¿ç•™ | âœ… éƒ¨åˆ†æ”¯æŒ | âŒ ç®€åŒ–å‚æ•° | âŒ æ— å‚æ•° | âŒ æ— å‚æ•° |
| **åˆ†é¡µæ”¯æŒ** | ğŸ”„ å¯æ‰©å±• | âœ… è‡ªåŠ¨åˆ†é¡µ | ğŸ”„ å¯æ‰©å±• | âŒ å•é¡µ | âŒ å•é¡µ |
| **å®ä½“åˆ†ç±»** | âœ… 12ç§ç±»å‹ | âŒ æ— åˆ†ç±» | âŒ æ— åˆ†ç±» | âœ… 12ç§ç±»å‹ | âŒ æ— åˆ†ç±» |
| **æ•°æ®æƒå¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ |

### ç‹¬ç‰¹ä¼˜åŠ¿

1. **ç»Ÿè®¡æ•°æ®ä¸“ä¸šæ€§**: ä¸“é—¨é’ˆå¯¹ç»Ÿè®¡æ•°æ®å’Œå¸‚åœºç ”ç©¶å†…å®¹
2. **å®ä½“ç±»å‹æ™ºèƒ½è¯†åˆ«**: è‡ªåŠ¨è¯†åˆ«å¹¶æ„å»ºæ­£ç¡®çš„URLæ ¼å¼
3. **å¤šå­—æ®µæ ‡é¢˜ä¼˜åŒ–**: çº§è”æå–æœ€ä½³æ ‡é¢˜å†…å®¹
4. **å‚æ•°å®Œæ•´é€ä¼ **: ä¿æŒç”¨æˆ·æœç´¢æ„å›¾ä¸å˜
5. **URLæ„å»ºå‡†ç¡®æ€§**: ç”Ÿæˆçš„é“¾æ¥ä¸å®˜æ–¹æ ¼å¼å®Œå…¨ä¸€è‡´

### åº”ç”¨åœºæ™¯

1. **å¸‚åœºç ”ç©¶**: è·å–è¡Œä¸šç»Ÿè®¡æ•°æ®å’Œè¶‹åŠ¿åˆ†æ
2. **å•†ä¸šå†³ç­–**: è®¿é—®æƒå¨çš„å¸‚åœºæ•°æ®æ”¯æŒå†³ç­–
3. **å­¦æœ¯ç ”ç©¶**: å¼•ç”¨å¯é çš„ç»Ÿè®¡æ•°æ®æº
4. **æŠ•èµ„åˆ†æ**: è·å–è¡Œä¸šå’Œå…¬å¸æ•°æ®
5. **æ–°é—»æŠ¥é“**: å¼•ç”¨æƒå¨ç»Ÿè®¡æ•°æ®

## æ‰©å±•å»ºè®®

### 1. åˆ†é¡µæ”¯æŒå®ç°

```python
async def _extract_statista_search_with_pagination(self, page, base_url, max_articles):
    all_articles = []
    current_page = 1
    
    while len(all_articles) < max_articles:
        # æ„å»ºå½“å‰é¡µå‚æ•°
        page_params = parse_qs(urlparse(base_url).query)
        page_params['p'] = [str(current_page)]
        
        # è°ƒç”¨å•é¡µAPI
        page_articles = await self._extract_single_page(page, page_params)
        if not page_articles:
            break
            
        all_articles.extend(page_articles)
        current_page += 1
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(1)
    
    return all_articles[:max_articles]
```

### 2. é«˜çº§è¿‡æ»¤æ”¯æŒ

```python
# æ”¯æŒæ›´å¤šè¿‡æ»¤æ¡ä»¶
advanced_filters = {
    'region': query_params.get('isoregion', ['0'])[0],     # åœ°ç†åŒºåŸŸ
    'timeframe': query_params.get('interval', ['0'])[0],   # æ—¶é—´èŒƒå›´
    'content_type': query_params.get('content', [''])[0],  # å†…å®¹ç±»å‹
    'industry': query_params.get('industry', [''])[0],     # è¡Œä¸šåˆ†ç±»
    'company': query_params.get('company', [''])[0]        # å…¬å¸ç­›é€‰
}

# æ·»åŠ åˆ°APIå‚æ•°ä¸­
for key, value in advanced_filters.items():
    if value:
        api_params.append((key, value))
```

### 3. æ•°æ®å¢å¼ºæå–

```python
# æå–æ›´å¤šå…ƒæ•°æ®
enhanced_article = {
    'title': title[:200],
    'url': article_url,
    'data_type': entity_name,                              # æ•°æ®ç±»å‹
    'content_id': item.get('idcontent'),                   # å†…å®¹ID
    'region': item.get('region_info'),                     # åœ°ç†ä¿¡æ¯
    'industry': item.get('industry_category'),             # è¡Œä¸šåˆ†ç±»
    'publication_date': item.get('published_date'),        # å‘å¸ƒæ—¥æœŸ
    'last_updated': item.get('last_updated'),              # æœ€åæ›´æ–°
    'data_points': item.get('data_point_count'),           # æ•°æ®ç‚¹æ•°é‡
    'chart_type': item.get('chart_type'),                  # å›¾è¡¨ç±»å‹
    'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S')
}
```

### 4. é”™è¯¯å¤„ç†å¢å¼º

```python
async def _robust_statista_api_call(self, page, target_url, max_retries=3):
    for attempt in range(max_retries):
        try:
            result = await page.evaluate("""
                async (url) => {
                    try {
                        const response = await fetch(url, {
                            headers: {
                                'Accept': 'application/json, text/plain, */*',
                                'X-Requested-With': 'XMLHttpRequest'
                            },
                            credentials: 'include',
                            timeout: 30000
                        });
                        return { status: response.status, text: await response.text() };
                    } catch (error) {
                        return { error: error.message };
                    }
                }
            """, target_url)
            
            if result.get('error'):
                raise Exception(f"Fetch error: {result['error']}")
                
            return result
            
        except Exception as e:
            logger.warning(f"Statista API attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                raise
```

### 5. ç¼“å­˜ä¼˜åŒ–

```python
import hashlib
from datetime import datetime, timedelta

class StatistaCache:
    def __init__(self, ttl_hours=6):
        self.cache = {}
        self.ttl = timedelta(hours=ttl_hours)
    
    def _get_cache_key(self, search_params):
        param_str = urllib.parse.urlencode(sorted(search_params))
        return hashlib.md5(param_str.encode()).hexdigest()
    
    def get(self, search_params):
        key = self._get_cache_key(search_params)
        if key in self.cache:
            data, timestamp = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                return data
        return None
    
    def set(self, search_params, data):
        key = self._get_cache_key(search_params)
        self.cache[key] = (data, datetime.now())
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired()
    
    def _cleanup_expired(self):
        now = datetime.now()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if now - timestamp >= self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]
```

## ç»´æŠ¤å»ºè®®

### 1. APIæ¥å£ç›‘æ§

```python
async def monitor_statista_api(self):
    test_url = "https://www.statista.com/search/?q=test&asJsonResponse=1"
    
    try:
        result = await self._test_api_call(test_url)
        
        # æ£€æŸ¥å“åº”ç»“æ„
        if not result.get('results', {}).get('mainselect'):
            logger.warning("Statista API structure may have changed")
            return False
            
        # æ£€æŸ¥å®ä½“æ˜ å°„
        entity_ids = result.get('justSmart', {}).get('actionParameters', {}).get('entityIds', {})
        if not entity_ids:
            logger.warning("Statista entity mapping structure changed")
            return False
            
        return True
        
    except Exception as e:
        logger.error(f"Statista API monitoring failed: {e}")
        return False
```

### 2. URLæ„å»ºéªŒè¯

```python
async def validate_statista_urls(self, articles):
    valid_count = 0
    
    for article in articles[:5]:  # æµ‹è¯•å‰5ä¸ªé“¾æ¥
        try:
            async with aiohttp.ClientSession() as session:
                async with session.head(article['url'], timeout=10) as response:
                    if response.status == 200:
                        valid_count += 1
                    else:
                        logger.warning(f"Invalid Statista URL: {article['url']} (status: {response.status})")
        except Exception as e:
            logger.warning(f"URL validation failed for {article['url']}: {e}")
    
    success_rate = valid_count / min(len(articles), 5)
    if success_rate < 0.8:
        logger.error(f"Statista URL validation success rate too low: {success_rate:.2%}")
    
    return success_rate >= 0.8
```

### 3. å®ä½“ç±»å‹æ›´æ–°

```python
# å®šæœŸæ£€æŸ¥æ–°çš„å®ä½“ç±»å‹
async def discover_new_entity_types(self, page):
    # æ‰§è¡Œå¤šç§æœç´¢è·å–ä¸åŒç±»å‹çš„å®ä½“
    test_queries = ['market study', 'forecast', 'infographic', 'survey']
    
    discovered_types = set()
    
    for query in test_queries:
        result = await self._call_api_for_discovery(page, query)
        entity_ids = result.get('justSmart', {}).get('actionParameters', {}).get('entityIds', {})
        discovered_types.update(entity_ids.keys())
    
    # æ¯”è¾ƒç°æœ‰æ˜ å°„
    existing_types = set(entity_to_path.keys())
    new_types = discovered_types - existing_types
    
    if new_types:
        logger.info(f"Discovered new Statista entity types: {new_types}")
        # å¯ä»¥å‘é€å‘Šè­¦æˆ–è‡ªåŠ¨æ›´æ–°é…ç½®
```

### 4. æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†URLæ„å»º
def batch_build_urls(self, items, id_to_name_map):
    urls = []
    
    for item in items:
        try:
            identity = str(item.get('identity', ''))
            entity_name = id_to_name_map.get(identity)
            url = self._build_statista_url(item, entity_name)
            if url:
                urls.append(url)
        except Exception as e:
            logger.debug(f"URL build failed for item {item.get('identity')}: {e}")
            continue
    
    return urls

# å¹¶å‘éªŒè¯URL
async def concurrent_url_validation(self, urls):
    async def validate_single_url(session, url):
        try:
            async with session.head(url, timeout=5) as response:
                return response.status == 200
        except:
            return False
    
    async with aiohttp.ClientSession() as session:
        tasks = [validate_single_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
    
    valid_urls = [url for url, valid in zip(urls, results) if valid]
    return valid_urls
```

## æ€»ç»“

Statistaçš„å®šåˆ¶åŒ–çˆ¬å–å®ç°é€šè¿‡"å†…éƒ¨API + æ™ºèƒ½URLæ„å»º"ç­–ç•¥ï¼ŒæˆåŠŸè§£å†³äº†ç»Ÿè®¡æ•°æ®å¹³å°çš„å¤æ‚æŠ€æœ¯æŒ‘æˆ˜ã€‚è¯¥å®ç°å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æŠ€æœ¯ä¼˜åŠ¿
- **APIç›´è¿**: ç»•è¿‡å‰ç«¯æ¸²æŸ“ï¼Œç›´æ¥è·å–ç»“æ„åŒ–æ•°æ®
- **æ™ºèƒ½URLæ„å»º**: æ ¹æ®å®ä½“ç±»å‹è‡ªåŠ¨ç”Ÿæˆæ­£ç¡®çš„è®¿é—®é“¾æ¥
- **å‚æ•°é€ä¼ **: å®Œæ•´ä¿ç•™ç”¨æˆ·æœç´¢æ„å›¾
- **å¤šå­—æ®µæ ‡é¢˜**: çº§è”æå–æœ€ä½³æ ‡é¢˜å†…å®¹
- **å®ä½“ç±»å‹è¯†åˆ«**: æ”¯æŒ12ç§ä¸åŒçš„ç»Ÿè®¡æ•°æ®ç±»å‹

### åº”ç”¨ä»·å€¼
- **æƒå¨æ•°æ®æº**: Statistaä½œä¸ºå…¨çƒé¢†å…ˆçš„ç»Ÿè®¡æ•°æ®å¹³å°
- **å¤šæ ·åŒ–å†…å®¹**: æ¶µç›–ç»Ÿè®¡å›¾è¡¨ã€å¸‚åœºç ”ç©¶ã€è¡Œä¸šæŠ¥å‘Šç­‰
- **ä¸“ä¸šæ€§å¼º**: ä¸ºå•†ä¸šå†³ç­–å’Œå­¦æœ¯ç ”ç©¶æä¾›å¯é æ•°æ®æ”¯æŒ
- **å…¨çƒè¦†ç›–**: æ¶µç›–å„å›½å„è¡Œä¸šçš„ç»Ÿè®¡æ•°æ®

è¯¥å®ç°ä¸ºWeRSSé¡¹ç›®æä¾›äº†é«˜è´¨é‡çš„ç»Ÿè®¡æ•°æ®å†…å®¹æºï¼Œä¸å…¶ä»–å®šåˆ¶çˆ¬è™«ä¸€èµ·æ„æˆäº†æ¶µç›–å­¦æœ¯ã€è¡Œä¸šã€æ–°é—»ã€æ•°æ®ç­‰å¤šä¸ªé¢†åŸŸçš„å®Œæ•´å†…å®¹æŠ“å–è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚åˆéœ€è¦æƒå¨ç»Ÿè®¡æ•°æ®æ”¯æŒçš„åº”ç”¨åœºæ™¯ã€‚