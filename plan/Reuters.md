# Reuters å®šåˆ¶åŒ–çˆ¬å–å®ç°è¯¦è§£

## å®ç°æ¦‚è¿°

### èƒŒæ™¯é—®é¢˜
Reutersï¼ˆè·¯é€ç¤¾ï¼‰ä½œä¸ºå…¨çƒé¢†å…ˆçš„æ–°é—»é€šè®¯ç¤¾ï¼Œå…¶æœç´¢é¡µé¢é‡‡ç”¨äº†å…ˆè¿›çš„åçˆ¬è™«ä¿æŠ¤æœºåˆ¶ï¼š

1. **DataDomeé˜²æŠ¤**: æ£€æµ‹headlessæµè§ˆå™¨å¹¶è¿”å›HTTP 401é”™è¯¯
2. **åŠ¨æ€å†…å®¹åŠ è½½**: æœç´¢ç»“æœé€šè¿‡JavaScriptè°ƒç”¨`articles-by-search-v2` APIåŠ¨æ€è·å–
3. **é€šç”¨é€‰æ‹©å™¨å¤±æ•ˆ**: ä¼ ç»ŸDOMé€‰æ‹©å™¨åªèƒ½æŠ“å–åˆ°é™æ€å¯¼èˆªæ ï¼Œæ— æ³•è·å–å®é™…æ–‡ç« 

### è§£å†³æ–¹æ¡ˆ
é‡‡ç”¨"åæ£€æµ‹ + APIç›´è¿"çš„åŒé‡ç­–ç•¥ï¼š
1. **åæ£€æµ‹**: æ£€æµ‹åˆ°ReutersåŸŸåæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æœ‰å¤´æ¨¡å¼ï¼Œç»•è¿‡DataDomeæ£€æµ‹
2. **APIç›´è¿**: åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ç›´æ¥è°ƒç”¨å®˜æ–¹æœç´¢APIï¼Œè·å–ç»“æ„åŒ–æ•°æ®

## æŠ€æœ¯æ¶æ„

### å‡½æ•°è°ƒç”¨é“¾

```
crawl_website(url, max_articles)
â”œâ”€â”€ crawler_instance.crawl_website_articles()
â”‚   â”œâ”€â”€ urlparse(url) è§£æç›®æ ‡URL
â”‚   â”œâ”€â”€ æ£€æµ‹reuters.comåŸŸå â†’ åˆ‡æ¢æœ‰å¤´æ¨¡å¼
â”‚   â”œâ”€â”€ async_playwright() å¯åŠ¨æœ‰å¤´æµè§ˆå™¨
â”‚   â”œâ”€â”€ page.goto(url) è®¿é—®æœç´¢é¡µé¢
â”‚   â””â”€â”€ _extract_articles(page, base_url, max_articles)
â”‚       â””â”€â”€ æ£€æµ‹reuters.comåŸŸå
â”‚           â””â”€â”€ _extract_reuters_search(page, base_url, max_articles)
â”‚               â”œâ”€â”€ parse_qs() è§£ææŸ¥è¯¢å‚æ•°
â”‚               â”œâ”€â”€ æ„å»ºAPIè¯·æ±‚è½½è·
â”‚               â”œâ”€â”€ page.evaluate() åœ¨æµè§ˆå™¨ä¸­æ‰§è¡Œfetch
â”‚               â”œâ”€â”€ json.loads() è§£æAPIå“åº”
â”‚               â””â”€â”€ æ„é€ æ ‡å‡†åŒ–æ–‡ç« è®°å½•
```

### æ ¸å¿ƒç­–ç•¥

1. **åŒé‡åŸŸåæ£€æµ‹**: æµè§ˆå™¨å¯åŠ¨å‰å’Œæ–‡ç« æå–æ—¶åˆ†åˆ«æ£€æµ‹
2. **æœ‰å¤´æ¨¡å¼ç»•è¿‡**: æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æµè§ˆå™¨è¡Œä¸º
3. **æµè§ˆå™¨å†…APIè°ƒç”¨**: åˆ©ç”¨é¡µé¢ä¸Šä¸‹æ–‡çš„è®¤è¯çŠ¶æ€
4. **å®˜æ–¹APIé›†æˆ**: ç›´æ¥è°ƒç”¨Reuterså†…éƒ¨æœç´¢æ¥å£

## æ ¸å¿ƒå®ç°

### 1. åDataDomeæ£€æµ‹æœºåˆ¶ (`core/crawler.py:52-56`)

```python
parsed_target_url = urlparse(url)
target_headless = self.headless
if parsed_target_url.netloc.endswith('reuters.com') and target_headless:
    target_headless = False
    logger.info('Reuters domain detected; switching to headful mode to satisfy anti-bot checks')
```

**å·¥ä½œåŸç†**:
- **é¢„æ£€æµ‹**: åœ¨æµè§ˆå™¨å¯åŠ¨å‰åˆ†æç›®æ ‡URL
- **åŠ¨æ€åˆ‡æ¢**: æ£€æµ‹åˆ°ReutersåŸŸåæ—¶å¼ºåˆ¶ä½¿ç”¨æœ‰å¤´æ¨¡å¼
- **é€æ˜å¤„ç†**: å¯¹ç”¨æˆ·é€æ˜ï¼Œè‡ªåŠ¨å¤„ç†åçˆ¬æ£€æµ‹
- **æ—¥å¿—è®°å½•**: è®°å½•åˆ‡æ¢è¡Œä¸ºä¾¿äºè°ƒè¯•

**DataDomeç»•è¿‡ç­–ç•¥**:
- **æœ‰å¤´æµè§ˆå™¨**: æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ç¯å¢ƒ
- **å®Œæ•´UA**: ä½¿ç”¨æ ‡å‡†Chrome User-Agent
- **Cookieæ”¯æŒ**: ä¿æŒå®Œæ•´çš„æµè§ˆå™¨ä¼šè¯çŠ¶æ€
- **JavaScriptæ‰§è¡Œ**: å…è®¸DataDomeè„šæœ¬æ­£å¸¸è¿è¡Œ

### 2. åŸŸåæ£€æµ‹ä¸è·¯ç”± (`core/crawler.py:655-658`)

```python
if parsed_url.netloc.endswith('reuters.com'):
    reuters_articles = await self._extract_reuters_search(page, base_url, max_articles)
    if reuters_articles:
        return reuters_articles
```

**è§¦å‘æ¡ä»¶**:
- **åŸŸååŒ¹é…**: `*.reuters.com` (æ”¯æŒå­åŸŸå)
- **æ— è·¯å¾„é™åˆ¶**: ä»»ä½•Reutersé¡µé¢éƒ½ä¼šè§¦å‘
- **ä¼˜å…ˆçº§**: ä¸­ç­‰ä¼˜å…ˆçº§ (åœ¨ä¸‡æ–¹å’ŒFoodNavigatorä¹‹åï¼ŒStatistaä¹‹å‰)

### 3. æŸ¥è¯¢å‚æ•°è§£æ (`core/crawler.py:422-431`)

```python
keyword = ''
for key in ('query', 'blob'):
    values = query_params.get(key)
    if values:
        keyword = (values[0] or '').strip()
        if keyword:
            break

if not keyword:
    return articles
```

**æ”¯æŒçš„å‚æ•°**:
- `query`: æ ‡å‡†æœç´¢å…³é”®è¯å‚æ•°
- `blob`: Reutersç‰¹æœ‰çš„æœç´¢å‚æ•°
- **å‚æ•°éªŒè¯**: æ— å…³é”®è¯æ—¶ç›´æ¥è¿”å›ç©ºç»“æœ

**URLç¤ºä¾‹**:
- `https://www.reuters.com/site-search/?query=flaxseed`
- `https://www.reuters.com/search/?blob=technology`

### 4. APIè¯·æ±‚è½½è·æ„å»º (`core/crawler.py:433-440`)

```python
size = max(1, min(max_articles, 20))
payload = {
    'keyword': keyword,
    'offset': 0,
    'orderby': 'display_date:desc',
    'size': size,
    'website': 'reuters'
}
```

**è½½è·å­—æ®µè¯´æ˜**:
- `keyword`: æœç´¢å…³é”®è¯
- `offset`: åˆ†é¡µåç§»é‡ (å½“å‰å›ºå®šä¸º0)
- `orderby`: æ’åºæ–¹å¼ (æŒ‰å‘å¸ƒæ—¥æœŸé™åº)
- `size`: è¿”å›æ–‡ç« æ•°é‡ (é™åˆ¶1-20)
- `website`: ç«™ç‚¹æ ‡è¯†ç¬¦

**åˆ†é¡µæ”¯æŒè®¾è®¡**:
```python
# æœªæ¥å¯æ‰©å±•çš„åˆ†é¡µé€»è¾‘
for offset in range(0, max_articles, 20):
    payload['offset'] = offset
    # æ‰§è¡ŒAPIè°ƒç”¨...
```

### 5. æµè§ˆå™¨å†…APIè°ƒç”¨ (`core/crawler.py:442-462`)

```python
fetch_script = """async ({url, payload}) => {
    const params = new URLSearchParams();
    params.set('query', JSON.stringify(payload));
    params.set('d', '318');
    params.set('mxId', '00000000');
    params.set('_website', 'reuters');
    const response = await fetch(`${url}?${params.toString()}`, {
        credentials: 'include',
        headers: {
            'Accept': 'application/json, text/plain, */*',
            'X-Requested-With': 'XMLHttpRequest'
        }
    });
    return { status: response.status, text: await response.text() };
}"""

fetch_result = await page.evaluate(fetch_script, {
    'url': 'https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2',
    'payload': payload
})
```

**æŠ€æœ¯è¦ç‚¹**:

**APIç«¯ç‚¹**: `https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2`

**å¿…éœ€å‚æ•°**:
- `query`: JSONåŒ–çš„æœç´¢è½½è·
- `d`: å›ºå®šå€¼ `318` (å¯èƒ½æ˜¯ç‰ˆæœ¬æˆ–é…ç½®ID)
- `mxId`: å›ºå®šå€¼ `00000000` (å¯èƒ½æ˜¯ä¼šè¯ID)
- `_website`: å›ºå®šå€¼ `reuters`

**å…³é”®è¯·æ±‚å¤´**:
- `Accept`: æŒ‡å®šæ¥å—JSONå“åº”
- `X-Requested-With`: æ ‡è¯†AJAXè¯·æ±‚
- `credentials: 'include'`: åŒ…å«è®¤è¯Cookie

**æµè§ˆå™¨å†…æ‰§è¡Œä¼˜åŠ¿**:
- **å…±äº«è®¤è¯**: åˆ©ç”¨é¡µé¢å·²æœ‰çš„ç™»å½•çŠ¶æ€å’ŒCookie
- **ç»•è¿‡CORS**: é¿å…è·¨åŸŸè¯·æ±‚é™åˆ¶
- **åæ£€æµ‹**: è¯·æ±‚æ¥æºäºçœŸå®æµè§ˆå™¨ç¯å¢ƒ
- **å®Œæ•´ä¸Šä¸‹æ–‡**: ç»§æ‰¿é¡µé¢çš„æ‰€æœ‰HTTPå¤´å’ŒçŠ¶æ€

### 6. å“åº”å¤„ç†ä¸é”™è¯¯æ§åˆ¶ (`core/crawler.py:467-480`)

```python
status = fetch_result.get('status') if isinstance(fetch_result, dict) else None
if status != 200:
    logger.warning('Reuters search API returned non-200 response: %s', status)
    return articles

raw_text = (fetch_result.get('text') or '').strip()
if not raw_text:
    return articles

try:
    data = json.loads(raw_text)
except json.JSONDecodeError as exc:
    logger.warning('Failed to decode Reuters search payload: %s', exc)
    return articles
```

**é”™è¯¯å¤„ç†ç­–ç•¥**:
- **çŠ¶æ€ç æ£€æŸ¥**: åªå¤„ç†HTTP 200å“åº”
- **ç©ºå“åº”è¿‡æ»¤**: è·³è¿‡ç©ºç™½å“åº”å†…å®¹
- **JSONè§£æä¿æŠ¤**: æ•è·æ ¼å¼é”™è¯¯å¹¶è®°å½•æ—¥å¿—
- **ä¼˜é›…é™çº§**: å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸å½±å“å…¶ä»–çˆ¬è™«

### 7. æ•°æ®æå–ä¸æ ‡å‡†åŒ– (`core/crawler.py:485-511`)

```python
items = data.get('result', {}).get('articles') or []
base_domain = 'https://www.reuters.com'

for item in items:
    if len(articles) >= max_articles:
        break

    # æ ‡é¢˜æå– (å¤šå­—æ®µå¤‡é€‰)
    title = (item.get('title') or item.get('headline') or '').strip()
    description = (item.get('description') or '').strip()
    if not title and description:
        title = description[:200]

    # URLæ„é€ 
    url_path = (item.get('canonical_url') or item.get('url') or '').strip()
    if not url_path:
        continue

    if url_path.startswith('http'):
        article_url = url_path
    else:
        article_url = urljoin(base_domain, url_path)

    # æ ‡é¢˜å›é€€
    if not title:
        title = article_url

    # æ ‡å‡†åŒ–è®°å½•
    articles.append({
        'title': title[:200],
        'url': article_url,
        'published_at': item.get('published_time'),
        'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S')
    })
```

**æ•°æ®å­—æ®µæ˜ å°„**:

| APIå­—æ®µ | è¯´æ˜ | å¤‡é€‰å­—æ®µ | è¾“å‡ºå­—æ®µ |
|---------|------|----------|----------|
| `title` | ä¸»æ ‡é¢˜ | `headline` | `title` |
| `description` | æè¿° | - | æ ‡é¢˜å¤‡é€‰ |
| `canonical_url` | è§„èŒƒURL | `url` | `url` |
| `published_time` | å‘å¸ƒæ—¶é—´ | - | `published_at` |

**æ•°æ®å¤„ç†ç‰¹ç‚¹**:
- **å¤šå­—æ®µå¤‡é€‰**: ä¼˜å…ˆä½¿ç”¨`title`ï¼Œå¤‡é€‰`headline`å’Œ`description`
- **URLæ ‡å‡†åŒ–**: æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
- **é•¿åº¦é™åˆ¶**: æ ‡é¢˜é™åˆ¶200å­—ç¬¦é˜²æ­¢è¿‡é•¿
- **å¿…å¡«éªŒè¯**: æ— URLçš„è®°å½•è¢«è·³è¿‡
- **æ—¶é—´æˆ³**: è®°å½•æ•°æ®æå–æ—¶é—´

## APIæ¥å£è¯¦è§£

### Reuters Search APIè§„èŒƒ

**ç«¯ç‚¹**: `https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2`

**è¯·æ±‚æ–¹æ³•**: GET

**å¿…éœ€å‚æ•°**:
- `query`: JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«æœç´¢è½½è·
- `d`: ç‰ˆæœ¬æ ‡è¯† (å›ºå®šå€¼: `318`)
- `mxId`: ä¼šè¯æ ‡è¯† (å›ºå®šå€¼: `00000000`)
- `_website`: ç«™ç‚¹æ ‡è¯† (å›ºå®šå€¼: `reuters`)

**è½½è·ç»“æ„** (queryå‚æ•°çš„JSONå†…å®¹):
```json
{
    "keyword": "æœç´¢å…³é”®è¯",
    "offset": 0,
    "orderby": "display_date:desc",
    "size": 20,
    "website": "reuters"
}
```

**å“åº”æ ¼å¼**:
```json
{
    "result": {
        "articles": [
            {
                "title": "æ–‡ç« æ ‡é¢˜",
                "headline": "å‰¯æ ‡é¢˜",
                "description": "æ–‡ç« æè¿°",
                "canonical_url": "/world/asia/article-path/",
                "url": "/world/asia/article-path/",
                "published_time": "2023-12-01T10:30:00Z"
            }
        ]
    }
}
```

### è®¤è¯ä¸æƒé™

**è®¤è¯æ–¹å¼**: Cookie-basedä¼šè¯è®¤è¯
- **æ— éœ€API Key**: ä¾èµ–æµè§ˆå™¨ä¼šè¯çŠ¶æ€
- **å…¬å¼€è®¿é—®**: æœç´¢åŠŸèƒ½å¯¹å…¬ä¼—å¼€æ”¾
- **åçˆ¬ä¿æŠ¤**: é€šè¿‡DataDomeè¿›è¡Œbotæ£€æµ‹

**è¯·æ±‚é™åˆ¶**:
- **é¢‘ç‡é™åˆ¶**: æœªæ˜ç¡®ï¼Œå»ºè®®æ§åˆ¶è¯·æ±‚é¢‘ç‡
- **å¤§å°é™åˆ¶**: å•æ¬¡æœ€å¤š20ç¯‡æ–‡ç« 
- **åœ°ç†é™åˆ¶**: å¯èƒ½å­˜åœ¨åœ°åŒºè®¿é—®é™åˆ¶

## éªŒè¯æ–¹æ³•

### å¿«é€ŸéªŒè¯
è¿è¡ŒéªŒè¯è„šæœ¬ï¼š
```bash
python verify_reuters.py
```

### éªŒè¯è„šæœ¬ç¤ºä¾‹
```python
import asyncio
from core.crawler import LinkCrawler

async def main():
    crawler = LinkCrawler(headless=True, timeout=120000)
    url = 'https://www.reuters.com/site-search/?query=flaxseed'
    result = await crawler.crawl_website_articles(url, max_articles=5)
    print('success:', result['success'])
    print('total:', result['total_found'])
    print('error:', result['error'])
    for idx, article in enumerate(result['articles'], 1):
        print(f"{idx}. {article['title']}")
        print(f"   URL: {article['url']}")
        print(f"   å‘å¸ƒæ—¶é—´: {article.get('published_at', 'N/A')}")
        print()

asyncio.run(main())
```

### é¢„æœŸè¾“å‡º
```
success: True
total: 4
error: None
1. India Gate basmati rice maker KRBL's Q3 profit soars on strong demand
   URL: https://www.reuters.com/world/india/india-gate-basmati-rice-maker-krbls-q3-profit-soars-strong-demand-2023-02-03/
   å‘å¸ƒæ—¶é—´: 2023-02-03T08:15:00Z

2. All rise! Judge the new AL home run king
   URL: https://www.reuters.com/lifestyle/sports/yankees-judge-breaks-maris-home-run-record-with-no-62-2022-10-05/
   å‘å¸ƒæ—¶é—´: 2022-10-05T21:30:00Z
...
```

### éªŒè¯è¦ç‚¹
- [ ] **åæ£€æµ‹åŠŸèƒ½**: ç¡®è®¤æœ‰å¤´æ¨¡å¼è‡ªåŠ¨åˆ‡æ¢
- [ ] **APIè°ƒç”¨æˆåŠŸ**: æ£€æŸ¥HTTP 200å“åº”
- [ ] **æ•°æ®å®Œæ•´æ€§**: éªŒè¯æ ‡é¢˜ã€URLã€æ—¶é—´å­—æ®µ
- [ ] **é“¾æ¥æœ‰æ•ˆæ€§**: ç¡®è®¤ç”Ÿæˆçš„URLå¯ä»¥æ­£å¸¸è®¿é—®
- [ ] **é”™è¯¯å¤„ç†**: æµ‹è¯•æ— æ•ˆå…³é”®è¯å’Œç½‘ç»œå¼‚å¸¸æƒ…å†µ

## æ¶æ„å¯¹æ¯”

### ä¸å…¶ä»–å®šåˆ¶çˆ¬è™«çš„å¯¹æ¯”

| ç‰¹æ€§ | Reuters | FoodNavigator | ä¸‡æ–¹æ•°æ® | Statista | é€šç”¨çˆ¬è™« |
|------|---------|---------------|----------|----------|-----------|
| **åçˆ¬ç­–ç•¥** | DataDomeæ£€æµ‹ | æ— ç‰¹æ®Šä¿æŠ¤ | åŠ¨æ€åŠ è½½ | å†…éƒ¨API | åŸºæœ¬é˜²æŠ¤ |
| **ç»•è¿‡æ–¹æ³•** | æœ‰å¤´æ¨¡å¼ | APIç›´è¿ | DOMç­‰å¾… | APIç›´è¿ | é€‰æ‹©å™¨ |
| **APIé›†æˆ** | âœ… å®˜æ–¹API | âœ… Queryly | âŒ DOMè§£æ | âœ… å†…éƒ¨API | âŒ æ— API |
| **è®¤è¯éœ€æ±‚** | Cookieä¼šè¯ | APIå¯†é’¥ | æ— éœ€è®¤è¯ | æ— éœ€è®¤è¯ | æ— éœ€è®¤è¯ |
| **åˆ†é¡µæ”¯æŒ** | ğŸ”„ å¯æ‰©å±• | âœ… è‡ªåŠ¨åˆ†é¡µ | âŒ å•é¡µ | âœ… å‚æ•°æ§åˆ¶ | âŒ å•é¡µ |
| **å…ƒæ•°æ®ä¸°å¯Œåº¦** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| **æŠ€æœ¯å¤æ‚åº¦** | é«˜ | ä¸­ | ä¸­ | ä¸­ | ä½ |
| **ç¨³å®šæ€§** | é«˜ | é«˜ | é«˜ | é«˜ | ä¸­ |

### ç‹¬ç‰¹ä¼˜åŠ¿

1. **åæ£€æµ‹èƒ½åŠ›**: è‡ªåŠ¨è¯†åˆ«å¹¶ç»•è¿‡DataDomeä¿æŠ¤
2. **å®˜æ–¹API**: ç›´æ¥ä½¿ç”¨Reuterså†…éƒ¨æœç´¢æ¥å£
3. **æ•°æ®æƒå¨æ€§**: è·¯é€ç¤¾æƒå¨æ–°é—»å†…å®¹
4. **å…¨çƒè¦†ç›–**: å›½é™…æ–°é—»çš„å…¨é¢è¦†ç›–
5. **å®æ—¶æ€§**: æ–°é—»æ›´æ–°åŠæ—¶

### æŠ€æœ¯æŒ‘æˆ˜

1. **åçˆ¬å‡çº§**: DataDomeç­–ç•¥å¯èƒ½æŒç»­æ›´æ–°
2. **APIå˜åŒ–**: å†…éƒ¨APIç«¯ç‚¹å’Œå‚æ•°å¯èƒ½è°ƒæ•´
3. **åœ°åŒºé™åˆ¶**: éƒ¨åˆ†åœ°åŒºå¯èƒ½æ— æ³•è®¿é—®
4. **æ€§èƒ½å¼€é”€**: æœ‰å¤´æ¨¡å¼æ¶ˆè€—æ›´å¤šèµ„æº

## æ‰©å±•å»ºè®®

### 1. åˆ†é¡µæ”¯æŒå®ç°

```python
async def _extract_reuters_search_with_pagination(self, page, base_url, max_articles):
    all_articles = []
    offset = 0
    batch_size = 20
    
    while len(all_articles) < max_articles:
        payload = {
            'keyword': keyword,
            'offset': offset,
            'orderby': 'display_date:desc',
            'size': min(batch_size, max_articles - len(all_articles)),
            'website': 'reuters'
        }
        
        # æ‰§è¡ŒAPIè°ƒç”¨
        batch_articles = await self._call_reuters_api(page, payload)
        if not batch_articles:
            break
            
        all_articles.extend(batch_articles)
        offset += batch_size
        
        # æ·»åŠ å»¶æ—¶é¿å…è¢«é™åˆ¶
        await asyncio.sleep(1)
    
    return all_articles[:max_articles]
```

### 2. é«˜çº§æœç´¢å‚æ•°æ”¯æŒ

```python
# æ”¯æŒæ›´å¤šæœç´¢å‚æ•°
advanced_payload = {
    'keyword': keyword,
    'offset': 0,
    'orderby': 'display_date:desc',  # relevance, display_date:asc
    'size': size,
    'website': 'reuters',
    'date_from': '2023-01-01',      # èµ·å§‹æ—¥æœŸ
    'date_to': '2023-12-31',        # ç»“æŸæ—¥æœŸ
    'section': 'technology',        # æ–°é—»åˆ†ç±»
    'location': 'asia'              # åœ°ç†èŒƒå›´
}
```

### 3. é”™è¯¯é‡è¯•æœºåˆ¶

```python
async def _call_reuters_api_with_retry(self, page, payload, max_retries=3):
    for attempt in range(max_retries):
        try:
            result = await self._call_reuters_api(page, payload)
            if result:
                return result
        except Exception as e:
            logger.warning(f"Reuters API attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    return []
```

### 4. ç¼“å­˜æœºåˆ¶

```python
import hashlib
from datetime import datetime, timedelta

class ReutersCache:
    def __init__(self, ttl_minutes=30):
        self.cache = {}
        self.ttl = timedelta(minutes=ttl_minutes)
    
    def get_cache_key(self, keyword, offset, size):
        key_str = f"{keyword}:{offset}:{size}"
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, keyword, offset, size):
        key = self.get_cache_key(keyword, offset, size)
        if key in self.cache:
            data, timestamp = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                return data
        return None
    
    def set(self, keyword, offset, size, data):
        key = self.get_cache_key(keyword, offset, size)
        self.cache[key] = (data, datetime.now())
```

## ç»´æŠ¤å»ºè®®

### 1. DataDomeç­–ç•¥ç›‘æ§

```python
# å®šæœŸæ£€æŸ¥DataDomeçŠ¶æ€
async def check_datadom_status(self, url):
    try:
        # å°è¯•headlessè®¿é—®
        headless_result = await self._test_access(url, headless=True)
        # å°è¯•æœ‰å¤´è®¿é—®
        headful_result = await self._test_access(url, headless=False)
        
        if headless_result != headful_result:
            logger.warning("DataDome blocking detected, headless mode blocked")
            return False
        return True
    except Exception as e:
        logger.error(f"DataDome check failed: {e}")
        return False
```

### 2. APIæ¥å£ç›‘æ§

```python
# ç›‘æ§APIæ¥å£å˜åŒ–
async def check_api_health(self):
    test_payload = {
        'keyword': 'test',
        'offset': 0,
        'orderby': 'display_date:desc',
        'size': 1,
        'website': 'reuters'
    }
    
    try:
        result = await self._call_reuters_api(test_payload)
        if not result:
            logger.warning("Reuters API health check failed")
            # å‘é€å‘Šè­¦é€šçŸ¥
        return len(result) > 0
    except Exception as e:
        logger.error(f"Reuters API health check error: {e}")
        return False
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
# æµè§ˆå™¨å¤ç”¨
class ReutersCrawler:
    def __init__(self):
        self.browser = None
        self.context = None
    
    async def get_page(self):
        if not self.browser:
            self.browser = await playwright.chromium.launch(headless=False)
            self.context = await self.browser.new_context()
        return await self.context.new_page()
    
    async def close(self):
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
```

### 4. æ—¥å¿—ä¸ç›‘æ§

```python
import time
from functools import wraps

def monitor_reuters_api(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            logger.info(f"Reuters API call completed in {duration:.2f}s, {len(result)} articles")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"Reuters API call failed after {duration:.2f}s: {e}")
            raise
    return wrapper
```

## æ€»ç»“

Reutersçš„å®šåˆ¶åŒ–çˆ¬å–å®ç°é€šè¿‡"åæ£€æµ‹ + APIç›´è¿"çš„ç­–ç•¥ï¼ŒæˆåŠŸçªç ´äº†DataDomeé˜²æŠ¤å¹¶ç›´æ¥è®¿é—®å®˜æ–¹æœç´¢APIã€‚è¿™ç§å®ç°æ–¹å¼å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æŠ€æœ¯ä¼˜åŠ¿
- **åæ£€æµ‹èƒ½åŠ›å¼º**: è‡ªåŠ¨ç»•è¿‡DataDomeä¿æŠ¤
- **æ•°æ®è´¨é‡é«˜**: ç›´æ¥ä½¿ç”¨å®˜æ–¹APIæ•°æ®
- **ç¨³å®šæ€§å¥½**: é¿å…äº†DOMç»“æ„å˜åŒ–çš„å½±å“
- **æ‰©å±•æ€§å¼º**: æ”¯æŒåˆ†é¡µã€è¿‡æ»¤ç­‰é«˜çº§åŠŸèƒ½

### åº”ç”¨ä»·å€¼
- **æƒå¨æ–°é—»æº**: è·¯é€ç¤¾ä½œä¸ºå›½é™…æƒå¨åª’ä½“
- **å®æ—¶æ€§å¼º**: æ–°é—»æ›´æ–°åŠæ—¶
- **è¦†ç›–é¢å¹¿**: å…¨çƒæ–°é—»çš„å…¨é¢è¦†ç›–
- **ä¸“ä¸šæ€§é«˜**: é‡‘èã€æ”¿æ²»ã€ç§‘æŠ€ç­‰ä¸“ä¸šé¢†åŸŸ

è¯¥å®ç°ä¸ºWeRSSé¡¹ç›®æä¾›äº†é«˜è´¨é‡çš„å›½é™…æ–°é—»å†…å®¹æºï¼Œä¸å…¶ä»–å®šåˆ¶çˆ¬è™«ä¸€èµ·æ„æˆäº†æ¶µç›–å­¦æœ¯ã€è¡Œä¸šã€æ–°é—»ã€æ•°æ®ç­‰å¤šä¸ªé¢†åŸŸçš„å®Œæ•´å†…å®¹æŠ“å–è§£å†³æ–¹æ¡ˆã€‚